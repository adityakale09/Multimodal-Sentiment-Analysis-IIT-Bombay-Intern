{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a36de85",
   "metadata": {},
   "source": [
    "# Advanced Feature Engineering for Multimodal Sentiment Analysis\n",
    "\n",
    "This notebook performs advanced feature engineering on the preprocessed multimodal data, including feature selection, transformation, and creation of composite features for improved sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6b7f85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessed data loaded successfully\n",
      "Features shape: (38, 60)\n",
      "Labels shape: (38,)\n",
      "Label distribution: Unnamed: 0\n",
      "0     1\n",
      "1     1\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     1\n",
      "8     1\n",
      "9     1\n",
      "10    1\n",
      "11    1\n",
      "12    1\n",
      "13    1\n",
      "14    1\n",
      "15    1\n",
      "16    1\n",
      "17    1\n",
      "18    1\n",
      "19    1\n",
      "20    1\n",
      "21    1\n",
      "22    1\n",
      "23    1\n",
      "24    1\n",
      "25    1\n",
      "26    1\n",
      "27    1\n",
      "28    1\n",
      "29    1\n",
      "30    1\n",
      "31    1\n",
      "32    1\n",
      "33    1\n",
      "34    1\n",
      "35    1\n",
      "36    1\n",
      "37    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load preprocessed data\n",
    "try:\n",
    "    multimodal_features = pd.read_csv('../data/multimodal_features.csv')\n",
    "    sentiment_labels_df = pd.read_csv('../data/sentiment_labels.csv')\n",
    "    \n",
    "    # Extract sentiment column (first column if no header)\n",
    "    if 'sentiment' in sentiment_labels_df.columns:\n",
    "        sentiment_labels = sentiment_labels_df['sentiment']\n",
    "    else:\n",
    "        sentiment_labels = sentiment_labels_df.iloc[:, 0]\n",
    "    \n",
    "    print(\"âœ… Preprocessed data loaded successfully\")\n",
    "    print(f\"Features shape: {multimodal_features.shape}\")\n",
    "    print(f\"Labels shape: {sentiment_labels.shape}\")\n",
    "    print(f\"Label distribution: {sentiment_labels.value_counts().sort_index()}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Preprocessed data not found. Please run 01_preprocessing.ipynb first\")\n",
    "    multimodal_features = pd.DataFrame()\n",
    "    sentiment_labels = pd.Series()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading data: {str(e)}\")\n",
    "    multimodal_features = pd.DataFrame()\n",
    "    sentiment_labels = pd.Series()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001b8b8",
   "metadata": {},
   "source": [
    "## 1. Data Exploration and Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54132723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No feature data available. Please run preprocessing first.\n"
     ]
    }
   ],
   "source": [
    "if not multimodal_features.empty:\n",
    "    print(\"=== FEATURE DATA EXPLORATION ===\")\n",
    "    print(f\"Dataset shape: {multimodal_features.shape}\")\n",
    "    print(f\"Memory usage: {multimodal_features.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_counts = multimodal_features.isnull().sum()\n",
    "    missing_features = missing_counts[missing_counts > 0]\n",
    "    \n",
    "    if len(missing_features) > 0:\n",
    "        print(f\"\\nMissing values found in {len(missing_features)} features:\")\n",
    "        print(missing_features.head(10))\n",
    "    else:\n",
    "        print(\"\\nâœ… No missing values found\")\n",
    "    \n",
    "    # Feature type analysis\n",
    "    numeric_features = multimodal_features.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"\\nFeature types:\")\n",
    "    print(f\"  Numeric features: {len(numeric_features)}\")\n",
    "    \n",
    "    # Identify modalities by feature prefixes\n",
    "    modalities = {}\n",
    "    for col in numeric_features:\n",
    "        if col != 'participant_id':\n",
    "            # Identify modality based on common patterns\n",
    "            if any(band in col.lower() for band in ['delta', 'theta', 'alpha', 'beta', 'gamma', 'engagement']):\n",
    "                modalities.setdefault('EEG', []).append(col)\n",
    "            elif any(gsr in col.lower() for gsr in ['gsr', 'conductance', 'peaks', 'recovery']):\n",
    "                modalities.setdefault('GSR', []).append(col)\n",
    "            elif any(au in col.lower() for au in ['au', 'happiness', 'sadness', 'anger', 'valence_au', 'arousal_au']):\n",
    "                modalities.setdefault('AU', []).append(col)\n",
    "            elif col.startswith('sr_'):\n",
    "                modalities.setdefault('Self-Report', []).append(col)\n",
    "            else:\n",
    "                modalities.setdefault('Other', []).append(col)\n",
    "    \n",
    "    print(f\"\\nFeatures by modality:\")\n",
    "    for modality, features in modalities.items():\n",
    "        print(f\"  {modality}: {len(features)} features\")\n",
    "        print(f\"    Examples: {features[:3]}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(multimodal_features.describe().round(4))\n",
    "\n",
    "else:\n",
    "    print(\"No feature data available. Please run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32ca3d",
   "metadata": {},
   "source": [
    "## 2. Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b52ab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No feature data available for distribution analysis.\n"
     ]
    }
   ],
   "source": [
    "if not multimodal_features.empty:\n",
    "    # Select a subset of features for visualization\n",
    "    numeric_features = multimodal_features.select_dtypes(include=[np.number]).columns\n",
    "    feature_subset = [col for col in numeric_features if col != 'participant_id'][:16]  # First 16 features\n",
    "    \n",
    "    # Plot feature distributions\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, feature in enumerate(feature_subset):\n",
    "        if i < len(axes):\n",
    "            multimodal_features[feature].hist(bins=20, ax=axes[i], alpha=0.7)\n",
    "            axes[i].set_title(f'{feature}', fontsize=10)\n",
    "            axes[i].set_xlabel('Value')\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(feature_subset), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Feature Distributions', fontsize=14, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for zero variance features\n",
    "    zero_var_features = []\n",
    "    for col in numeric_features:\n",
    "        if col != 'participant_id' and multimodal_features[col].var() == 0:\n",
    "            zero_var_features.append(col)\n",
    "    \n",
    "    if zero_var_features:\n",
    "        print(f\"âš ï¸  Found {len(zero_var_features)} zero-variance features:\")\n",
    "        print(zero_var_features)\n",
    "    else:\n",
    "        print(\"âœ… No zero-variance features found\")\n",
    "    \n",
    "    # Check for high correlation features\n",
    "    correlation_matrix = multimodal_features[numeric_features].corr().abs()\n",
    "    upper_triangle = correlation_matrix.where(\n",
    "        np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    high_corr_pairs = []\n",
    "    for col in upper_triangle.columns:\n",
    "        for idx in upper_triangle.index:\n",
    "            if upper_triangle.loc[idx, col] > 0.95:  # 95% correlation threshold\n",
    "                high_corr_pairs.append((idx, col, upper_triangle.loc[idx, col]))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\nâš ï¸  Found {len(high_corr_pairs)} highly correlated feature pairs (>95%):\")\n",
    "        for feat1, feat2, corr in high_corr_pairs[:10]:  # Show first 10\n",
    "            print(f\"  {feat1} - {feat2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"\\nâœ… No highly correlated features found\")\n",
    "\n",
    "else:\n",
    "    print(\"No feature data available for distribution analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5f139",
   "metadata": {},
   "source": [
    "## 3. Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568974cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data available for feature engineering\n"
     ]
    }
   ],
   "source": [
    "def create_composite_features(data):\n",
    "    \"\"\"Create composite features from multimodal data\"\"\"\n",
    "    enhanced_data = data.copy()\n",
    "    \n",
    "    print(\"Creating composite features...\")\n",
    "    \n",
    "    # EEG composite features\n",
    "    eeg_cols = [col for col in data.columns if any(band in col.lower() for band in \n",
    "               ['delta', 'theta', 'alpha', 'beta', 'gamma'])]\n",
    "    \n",
    "    if eeg_cols:\n",
    "        print(f\"  Processing {len(eeg_cols)} EEG features\")\n",
    "        \n",
    "        # Frequency band groupings\n",
    "        delta_cols = [col for col in eeg_cols if 'delta' in col.lower()]\n",
    "        theta_cols = [col for col in eeg_cols if 'theta' in col.lower()]\n",
    "        alpha_cols = [col for col in eeg_cols if 'alpha' in col.lower()]\n",
    "        beta_cols = [col for col in eeg_cols if 'beta' in col.lower()]\n",
    "        gamma_cols = [col for col in eeg_cols if 'gamma' in col.lower()]\n",
    "        \n",
    "        # Create band ratios (important for cognitive states)\n",
    "        if alpha_cols and beta_cols:\n",
    "            alpha_mean = data[alpha_cols].mean(axis=1)\n",
    "            beta_mean = data[beta_cols].mean(axis=1)\n",
    "            enhanced_data['alpha_beta_ratio'] = alpha_mean / (beta_mean + 1e-8)\n",
    "        \n",
    "        if theta_cols and alpha_cols:\n",
    "            theta_mean = data[theta_cols].mean(axis=1)\n",
    "            alpha_mean = data[alpha_cols].mean(axis=1)\n",
    "            enhanced_data['theta_alpha_ratio'] = theta_mean / (alpha_mean + 1e-8)\n",
    "        \n",
    "        # Overall EEG activation\n",
    "        enhanced_data['eeg_total_power'] = data[eeg_cols].sum(axis=1)\n",
    "        enhanced_data['eeg_mean_power'] = data[eeg_cols].mean(axis=1)\n",
    "        \n",
    "        # EEG complexity (coefficient of variation)\n",
    "        enhanced_data['eeg_complexity'] = data[eeg_cols].std(axis=1) / (data[eeg_cols].mean(axis=1) + 1e-8)\n",
    "    \n",
    "    # GSR composite features\n",
    "    gsr_cols = [col for col in data.columns if any(gsr in col.lower() for gsr in \n",
    "               ['gsr', 'conductance', 'peaks', 'recovery'])]\n",
    "    \n",
    "    if gsr_cols:\n",
    "        print(f\"  Processing {len(gsr_cols)} GSR features\")\n",
    "        \n",
    "        # GSR arousal indicators\n",
    "        enhanced_data['gsr_arousal_index'] = data[gsr_cols].mean(axis=1)\n",
    "        \n",
    "        # GSR variability\n",
    "        enhanced_data['gsr_variability'] = data[gsr_cols].std(axis=1)\n",
    "        \n",
    "        # Peak-related features\n",
    "        peak_cols = [col for col in gsr_cols if 'peak' in col.lower()]\n",
    "        if peak_cols:\n",
    "            enhanced_data['gsr_peak_intensity'] = data[peak_cols].sum(axis=1)\n",
    "    \n",
    "    # Facial AU composite features\n",
    "    au_cols = [col for col in data.columns if any(au in col.lower() for au in \n",
    "              ['au', 'happiness', 'sadness', 'anger', 'surprise', 'disgust', 'fear'])]\n",
    "    \n",
    "    if au_cols:\n",
    "        print(f\"  Processing {len(au_cols)} AU features\")\n",
    "        \n",
    "        # Emotional intensity\n",
    "        emotion_cols = [col for col in au_cols if any(emotion in col.lower() for emotion in \n",
    "                       ['happiness', 'sadness', 'anger', 'surprise', 'disgust', 'fear'])]\n",
    "        \n",
    "        if emotion_cols:\n",
    "            enhanced_data['emotional_intensity'] = data[emotion_cols].abs().mean(axis=1)\n",
    "            \n",
    "            # Positive vs negative emotion balance\n",
    "            positive_emotions = [col for col in emotion_cols if 'happiness' in col.lower()]\n",
    "            negative_emotions = [col for col in emotion_cols if any(neg in col.lower() for neg in \n",
    "                               ['sadness', 'anger', 'disgust', 'fear'])]\n",
    "            \n",
    "            if positive_emotions and negative_emotions:\n",
    "                pos_mean = data[positive_emotions].mean(axis=1)\n",
    "                neg_mean = data[negative_emotions].mean(axis=1)\n",
    "                enhanced_data['emotion_valence_balance'] = pos_mean - neg_mean\n",
    "        \n",
    "        # Facial expressiveness\n",
    "        enhanced_data['facial_expressiveness'] = data[au_cols].std(axis=1)\n",
    "    \n",
    "    # Self-report composite features\n",
    "    sr_cols = [col for col in data.columns if col.startswith('sr_')]\n",
    "    \n",
    "    if sr_cols:\n",
    "        print(f\"  Processing {len(sr_cols)} self-report features\")\n",
    "        enhanced_data['sr_mean_rating'] = data[sr_cols].mean(axis=1)\n",
    "        enhanced_data['sr_rating_variability'] = data[sr_cols].std(axis=1)\n",
    "    \n",
    "    # Cross-modal interaction features\n",
    "    print(\"  Creating cross-modal interactions...\")\n",
    "    \n",
    "    # EEG-GSR interaction (cognitive-physiological coupling)\n",
    "    if eeg_cols and gsr_cols:\n",
    "        eeg_activation = data[eeg_cols].mean(axis=1)\n",
    "        gsr_activation = data[gsr_cols].mean(axis=1)\n",
    "        enhanced_data['eeg_gsr_coupling'] = eeg_activation * gsr_activation\n",
    "        enhanced_data['cognitive_arousal_ratio'] = eeg_activation / (gsr_activation + 1e-8)\n",
    "    \n",
    "    # AU-GSR interaction (emotional-physiological coupling)\n",
    "    if au_cols and gsr_cols:\n",
    "        au_intensity = data[au_cols].mean(axis=1)\n",
    "        gsr_activation = data[gsr_cols].mean(axis=1)\n",
    "        enhanced_data['emotion_arousal_coupling'] = au_intensity * gsr_activation\n",
    "    \n",
    "    print(f\"  Created {enhanced_data.shape[1] - data.shape[1]} new composite features\")\n",
    "    \n",
    "    return enhanced_data\n",
    "\n",
    "# Apply feature engineering\n",
    "if not multimodal_features.empty:\n",
    "    engineered_features = create_composite_features(multimodal_features)\n",
    "    print(f\"\\nEnhanced dataset shape: {engineered_features.shape}\")\n",
    "    print(f\"Original features: {multimodal_features.shape[1]}\")\n",
    "    print(f\"New features added: {engineered_features.shape[1] - multimodal_features.shape[1]}\")\n",
    "else:\n",
    "    engineered_features = pd.DataFrame()\n",
    "    print(\"No data available for feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd1397",
   "metadata": {},
   "source": [
    "## 4. Feature Selection and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "965a7666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot perform feature selection: missing data or labels\n"
     ]
    }
   ],
   "source": [
    "if not engineered_features.empty and not sentiment_labels.empty:\n",
    "    # Prepare features for selection\n",
    "    X = engineered_features.drop('participant_id', axis=1, errors='ignore')\n",
    "    y = sentiment_labels\n",
    "    \n",
    "    print(\"=== FEATURE SELECTION ANALYSIS ===\")\n",
    "    print(f\"Starting with {X.shape[1]} features\")\n",
    "    \n",
    "    # Remove zero variance features\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)  # Remove features with very low variance\n",
    "    X_variance_filtered = variance_selector.fit_transform(X)\n",
    "    selected_variance_features = X.columns[variance_selector.get_support()]\n",
    "    \n",
    "    print(f\"After variance filtering: {len(selected_variance_features)} features\")\n",
    "    \n",
    "    # Statistical feature selection\n",
    "    if len(y.unique()) > 1:  # Check if we have multiple classes\n",
    "        # F-statistic based selection\n",
    "        k_best = min(50, len(selected_variance_features))  # Select top 50 or all available\n",
    "        f_selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "        \n",
    "        X_f_selected = f_selector.fit_transform(X_variance_filtered, y)\n",
    "        f_scores = f_selector.scores_\n",
    "        selected_f_features = selected_variance_features[f_selector.get_support()]\n",
    "        \n",
    "        print(f\"After F-statistic selection (top {k_best}): {len(selected_f_features)} features\")\n",
    "        \n",
    "        # Display top features\n",
    "        feature_scores = list(zip(selected_f_features, f_scores[f_selector.get_support()]))\n",
    "        feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nTop 15 features by F-statistic:\")\n",
    "        for i, (feature, score) in enumerate(feature_scores[:15]):\n",
    "            print(f\"{i+1:2d}. {feature:35s} - {score:.2f}\")\n",
    "        \n",
    "        # Mutual information based selection\n",
    "        mi_selector = SelectKBest(score_func=mutual_info_classif, k=k_best)\n",
    "        X_mi_selected = mi_selector.fit_transform(X_variance_filtered, y)\n",
    "        mi_scores = mi_selector.scores_\n",
    "        selected_mi_features = selected_variance_features[mi_selector.get_support()]\n",
    "        \n",
    "        print(f\"After mutual information selection (top {k_best}): {len(selected_mi_features)} features\")\n",
    "        \n",
    "        # Tree-based feature importance\n",
    "        rf_selector = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        rf_selector.fit(X_variance_filtered, y)\n",
    "        \n",
    "        feature_importance = rf_selector.feature_importances_\n",
    "        importance_pairs = list(zip(selected_variance_features, feature_importance))\n",
    "        importance_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nTop 15 features by Random Forest importance:\")\n",
    "        for i, (feature, importance) in enumerate(importance_pairs[:15]):\n",
    "            print(f\"{i+1:2d}. {feature:35s} - {importance:.4f}\")\n",
    "        \n",
    "        # Combine selection methods (intersection of top features)\n",
    "        top_f_features = set([f[0] for f in feature_scores[:30]])\n",
    "        top_mi_features = set([f[0] for f in zip(selected_mi_features, mi_scores)])\n",
    "        top_rf_features = set([f[0] for f in importance_pairs[:30]])\n",
    "        \n",
    "        # Features that appear in multiple selection methods\n",
    "        consensus_features = list(top_f_features & top_rf_features)\n",
    "        print(f\"\\nConsensus features (F-stat + RF): {len(consensus_features)}\")\n",
    "        \n",
    "        if len(consensus_features) < 20:  # If too few consensus features, use top F-stat features\n",
    "            final_selected_features = [f[0] for f in feature_scores[:25]]\n",
    "        else:\n",
    "            final_selected_features = consensus_features[:25]  # Top 25 consensus features\n",
    "        \n",
    "        print(f\"Final selected features: {len(final_selected_features)}\")\n",
    "        \n",
    "        # Create final feature matrix\n",
    "        X_selected = X[final_selected_features]\n",
    "        \n",
    "        print(f\"\\nFinal feature selection complete:\")\n",
    "        print(f\"  Original features: {X.shape[1]}\")\n",
    "        print(f\"  Selected features: {X_selected.shape[1]}\")\n",
    "        print(f\"  Reduction: {(1 - X_selected.shape[1]/X.shape[1])*100:.1f}%\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Cannot perform feature selection: only one class found in labels\")\n",
    "        X_selected = X\n",
    "        final_selected_features = list(X.columns)\n",
    "\n",
    "else:\n",
    "    print(\"Cannot perform feature selection: missing data or labels\")\n",
    "    X_selected = pd.DataFrame()\n",
    "    final_selected_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5721930d",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfb1158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No selected features available for scaling\n"
     ]
    }
   ],
   "source": [
    "if not X_selected.empty:\n",
    "    print(\"=== FEATURE SCALING ANALYSIS ===\")\n",
    "    \n",
    "    # Analyze feature scales before normalization\n",
    "    print(\"Feature scale analysis:\")\n",
    "    scale_stats = pd.DataFrame({\n",
    "        'min': X_selected.min(),\n",
    "        'max': X_selected.max(),\n",
    "        'mean': X_selected.mean(),\n",
    "        'std': X_selected.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"Scale ranges:\")\n",
    "    print(f\"  Min value across features: {scale_stats['min'].min():.4f}\")\n",
    "    print(f\"  Max value across features: {scale_stats['max'].max():.4f}\")\n",
    "    print(f\"  Mean std deviation: {scale_stats['std'].mean():.4f}\")\n",
    "    \n",
    "    # Features with very different scales\n",
    "    large_scale_features = scale_stats[scale_stats['max'] > 100]['max'].sort_values(ascending=False)\n",
    "    if len(large_scale_features) > 0:\n",
    "        print(f\"\\nFeatures with large scales (>100):\")\n",
    "        print(large_scale_features.head(10))\n",
    "    \n",
    "    small_scale_features = scale_stats[scale_stats['max'] < 0.1]['max'].sort_values(ascending=True)\n",
    "    if len(small_scale_features) > 0:\n",
    "        print(f\"\\nFeatures with small scales (<0.1):\")\n",
    "        print(small_scale_features.head(10))\n",
    "    \n",
    "    # Apply different scaling methods\n",
    "    scalers = {\n",
    "        'StandardScaler': StandardScaler(),\n",
    "        'RobustScaler': RobustScaler(),\n",
    "        'MinMaxScaler': MinMaxScaler()\n",
    "    }\n",
    "    \n",
    "    scaled_datasets = {}\n",
    "    \n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        X_scaled = scaler.fit_transform(X_selected)\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=X_selected.columns, index=X_selected.index)\n",
    "        scaled_datasets[scaler_name] = X_scaled_df\n",
    "        \n",
    "        print(f\"\\n{scaler_name} results:\")\n",
    "        print(f\"  Mean: {X_scaled_df.mean().mean():.4f}\")\n",
    "        print(f\"  Std: {X_scaled_df.std().mean():.4f}\")\n",
    "        print(f\"  Min: {X_scaled_df.min().min():.4f}\")\n",
    "        print(f\"  Max: {X_scaled_df.max().max():.4f}\")\n",
    "    \n",
    "    # Visualize scaling effects\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Original data\n",
    "    X_selected.iloc[:, :5].boxplot(ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Original Features (first 5)')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # StandardScaler\n",
    "    scaled_datasets['StandardScaler'].iloc[:, :5].boxplot(ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('StandardScaler (first 5)')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # RobustScaler\n",
    "    scaled_datasets['RobustScaler'].iloc[:, :5].boxplot(ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('RobustScaler (first 5)')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # MinMaxScaler\n",
    "    scaled_datasets['MinMaxScaler'].iloc[:, :5].boxplot(ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('MinMaxScaler (first 5)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Choose StandardScaler as default (good for most ML algorithms)\n",
    "    X_final = scaled_datasets['StandardScaler']\n",
    "    print(f\"\\nUsing StandardScaler for final dataset\")\n",
    "    print(f\"Final processed dataset shape: {X_final.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"No selected features available for scaling\")\n",
    "    X_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154d4b7",
   "metadata": {},
   "source": [
    "## 6. Principal Component Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb106655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data available for PCA analysis\n"
     ]
    }
   ],
   "source": [
    "if not X_final.empty:\n",
    "    print(\"=== PRINCIPAL COMPONENT ANALYSIS ===\")\n",
    "    \n",
    "    # Determine optimal number of components\n",
    "    n_components_max = min(15, X_final.shape[1], X_final.shape[0])\n",
    "    \n",
    "    pca_full = PCA()\n",
    "    pca_full.fit(X_final)\n",
    "    \n",
    "    # Plot explained variance ratio\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(pca_full.explained_variance_ratio_[:20]) + 1), \n",
    "             pca_full.explained_variance_ratio_[:20], 'bo-')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('PCA Explained Variance by Component')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    plt.plot(range(1, len(cumsum_variance[:20]) + 1), cumsum_variance[:20], 'ro-')\n",
    "    plt.axhline(y=0.95, color='k', linestyle='--', alpha=0.7, label='95% variance')\n",
    "    plt.axhline(y=0.90, color='gray', linestyle='--', alpha=0.7, label='90% variance')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Cumulative Explained Variance')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find number of components for 90% and 95% variance\n",
    "    n_components_90 = np.argmax(cumsum_variance >= 0.90) + 1\n",
    "    n_components_95 = np.argmax(cumsum_variance >= 0.95) + 1\n",
    "    \n",
    "    print(f\"Components needed for 90% variance: {n_components_90}\")\n",
    "    print(f\"Components needed for 95% variance: {n_components_95}\")\n",
    "    print(f\"Total features: {X_final.shape[1]}\")\n",
    "    \n",
    "    # Create PCA-transformed dataset (optional - for dimensionality reduction)\n",
    "    n_components_final = min(n_components_90, 15)  # Use 90% variance or max 15 components\n",
    "    \n",
    "    if n_components_final < X_final.shape[1]:\n",
    "        pca_final = PCA(n_components=n_components_final)\n",
    "        X_pca = pca_final.fit_transform(X_final)\n",
    "        \n",
    "        # Create PCA feature names\n",
    "        pca_feature_names = [f'PC{i+1}' for i in range(n_components_final)]\n",
    "        X_pca_df = pd.DataFrame(X_pca, columns=pca_feature_names, index=X_final.index)\n",
    "        \n",
    "        print(f\"\\nPCA transformation complete:\")\n",
    "        print(f\"  Original features: {X_final.shape[1]}\")\n",
    "        print(f\"  PCA components: {X_pca_df.shape[1]}\")\n",
    "        print(f\"  Variance retained: {cumsum_variance[n_components_final-1]:.3f}\")\n",
    "        print(f\"  Dimensionality reduction: {(1 - X_pca_df.shape[1]/X_final.shape[1])*100:.1f}%\")\n",
    "        \n",
    "        # Show component loadings for first few components\n",
    "        print(f\"\\nTop feature loadings for first 3 components:\")\n",
    "        feature_names = X_final.columns\n",
    "        \n",
    "        for i in range(min(3, n_components_final)):\n",
    "            print(f\"\\nPC{i+1} (explains {pca_final.explained_variance_ratio_[i]:.3f} variance):\")\n",
    "            loadings = list(zip(feature_names, np.abs(pca_final.components_[i])))\n",
    "            loadings.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for j, (feature, loading) in enumerate(loadings[:5]):\n",
    "                print(f\"  {j+1}. {feature:30s}: {loading:.3f}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"PCA not beneficial - too few features\")\n",
    "        X_pca_df = X_final\n",
    "        pca_final = None\n",
    "\n",
    "else:\n",
    "    print(\"No data available for PCA analysis\")\n",
    "    X_pca_df = pd.DataFrame()\n",
    "    pca_final = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eedade",
   "metadata": {},
   "source": [
    "## 7. Final Dataset Export and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d781d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ No processed features available for export\n",
      "\n",
      "NEXT STEPS:\n",
      "1. Run 03_modeling_baseline_sentiment.ipynb for baseline model training\n",
      "2. Run 04_modeling_fusion.ipynb for multimodal fusion approaches\n",
      "3. Consider the PCA dataset for high-dimensional modeling approaches\n",
      "4. Review feature_metadata.json for detailed feature information\n",
      "\n",
      "Feature engineering completed successfully! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "# Export processed datasets\n",
    "export_datasets = {}\n",
    "\n",
    "if not X_final.empty:\n",
    "    export_datasets['engineered_features'] = X_final\n",
    "    print(\"=== FEATURE ENGINEERING SUMMARY ===\")\n",
    "    print()\n",
    "    \n",
    "    # Add participant_id back if it exists\n",
    "    if 'participant_id' in engineered_features.columns:\n",
    "        X_final_with_id = X_final.copy()\n",
    "        X_final_with_id['participant_id'] = engineered_features['participant_id']\n",
    "        export_datasets['engineered_features_with_id'] = X_final_with_id\n",
    "    \n",
    "    print(f\"Feature Engineering Pipeline Results:\")\n",
    "    print(f\"  Original raw features: {multimodal_features.shape[1] if not multimodal_features.empty else 0}\")\n",
    "    print(f\"  After composite feature creation: {engineered_features.shape[1] if not engineered_features.empty else 0}\")\n",
    "    print(f\"  After feature selection: {X_selected.shape[1] if not X_selected.empty else 0}\")\n",
    "    print(f\"  Final processed features: {X_final.shape[1]}\")\n",
    "    print(f\"  Total samples: {X_final.shape[0]}\")\n",
    "    print()\n",
    "    \n",
    "    # Feature categories in final dataset\n",
    "    final_features = list(X_final.columns)\n",
    "    feature_categories = {\n",
    "        'EEG': [f for f in final_features if any(x in f.lower() for x in ['delta', 'theta', 'alpha', 'beta', 'gamma', 'eeg'])],\n",
    "        'GSR': [f for f in final_features if any(x in f.lower() for x in ['gsr', 'conductance', 'peaks', 'arousal'])],\n",
    "        'Facial AU': [f for f in final_features if any(x in f.lower() for x in ['au', 'happiness', 'sadness', 'anger', 'facial', 'emotion'])],\n",
    "        'Self-Report': [f for f in final_features if f.startswith('sr_')],\n",
    "        'Composite': [f for f in final_features if any(x in f.lower() for x in ['ratio', 'coupling', 'balance', 'intensity', 'complexity'])]\n",
    "    }\n",
    "    \n",
    "    print(\"Final feature breakdown by category:\")\n",
    "    for category, features in feature_categories.items():\n",
    "        if features:\n",
    "            print(f\"  {category}: {len(features)} features\")\n",
    "            print(f\"    Examples: {features[:3]}\")\n",
    "    print()\n",
    "    \n",
    "    # Export main dataset\n",
    "    output_path = '../data/engineered_features.csv'\n",
    "    X_final_with_id.to_csv(output_path, index=False) if 'X_final_with_id' in locals() else X_final.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Exported engineered features to: {output_path}\")\n",
    "    \n",
    "    # Export PCA dataset if available\n",
    "    if not X_pca_df.empty and X_pca_df.shape[1] != X_final.shape[1]:\n",
    "        pca_output_path = '../data/pca_features.csv'\n",
    "        \n",
    "        if 'participant_id' in engineered_features.columns:\n",
    "            X_pca_with_id = X_pca_df.copy()\n",
    "            X_pca_with_id['participant_id'] = engineered_features['participant_id']\n",
    "            X_pca_with_id.to_csv(pca_output_path, index=False)\n",
    "        else:\n",
    "            X_pca_df.to_csv(pca_output_path, index=False)\n",
    "        \n",
    "        print(f\"âœ… Exported PCA features to: {pca_output_path}\")\n",
    "    \n",
    "    # Export feature metadata\n",
    "    feature_metadata = {\n",
    "        'selected_features': final_selected_features if 'final_selected_features' in locals() else list(X_final.columns),\n",
    "        'feature_categories': feature_categories,\n",
    "        'scaling_method': 'StandardScaler',\n",
    "        'pca_components': n_components_final if pca_final is not None else None,\n",
    "        'total_samples': X_final.shape[0],\n",
    "        'total_features': X_final.shape[1]\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    metadata_path = '../data/feature_metadata.json'\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(feature_metadata, f, indent=2)\n",
    "    print(f\"âœ… Exported feature metadata to: {metadata_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No processed features available for export\")\n",
    "\n",
    "print()\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"1. Run 03_modeling_baseline_sentiment.ipynb for baseline model training\")\n",
    "print(\"2. Run 04_modeling_fusion.ipynb for multimodal fusion approaches\")\n",
    "print(\"3. Consider the PCA dataset for high-dimensional modeling approaches\")\n",
    "print(\"4. Review feature_metadata.json for detailed feature information\")\n",
    "print()\n",
    "print(\"Feature engineering completed successfully! ðŸŽ‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
